{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetle + Seb code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import lasio as ls\n",
    "#import selenium\n",
    "#import phantomjs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "#import pyarrow.parquet as pq\n",
    "import fastparquet\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "import hvplot.dask\n",
    "from hvplot import hvPlot\n",
    "import holoviews as hv\n",
    "from holoviews import opts, streams\n",
    "from holoviews.plotting.links import DataLink\n",
    "hv.extension('bokeh', logo=None)\n",
    "from IPython.display import display, HTML\n",
    "from bokeh.models.tools import LassoSelectTool, BoxSelectTool  # tools=['hover']\n",
    "box_select = BoxSelectTool()\n",
    "lasso_select = LassoSelectTool()\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "client_secret = \"Bw2w*31.CmbIiP.i2EILQ=HN@xr]yeu?\"\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 80%; }\n",
    "    div#maintoolbar-container { width: 80%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../Documentation/Well log LAS files/WKF W8A LWD VISION RM 0_5m Drilling.LAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def las_to_df(filepath):\n",
    "    las = ls.read(filepath)\n",
    "    curve_df = las.df()\n",
    "    curve_df = curve_df.replace(-999.25,np.nan)\n",
    "    curve_df.reset_index(inplace=True)\n",
    "    curve_df.rename(columns={curve_df.columns[0]:'Depth'}, inplace=True)\n",
    "    las.curves[0].mnemonic = 'Depth'\n",
    "    return curve_df, las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realwell, las_realwell = las_to_df(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df only containing relevant curves (9) within a start and stop depth:\n",
    "def get_curves(df, curvenames, start_depth, stop_depth):\n",
    "    df_new = df.loc[start_depth<=df['Depth']].loc[df['Depth']<stop_depth][curvenames]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_curves = ['Depth', 'GR_ARC', 'A16H', 'A28H', 'A40H', 'TNPH', 'ROP5_RM', 'RHOB', 'DCAV'] # Relevant curve names for the chosen well\n",
    "\n",
    "start_depth = df_realwell['Depth'].iloc[0]\n",
    "end_depth = df_realwell['Depth'].iloc[-1]\n",
    "step_depth = df_realwell['Depth'].iloc[1] - df_realwell['Depth'].iloc[0]\n",
    "\n",
    "df_curves = get_curves(df_realwell, all_curves, start_depth, end_depth) # New df containing specific curves and start and stop depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_files = int(input(\"Number of files to generate: \"))\n",
    "depth_start = int(input(\"Depth reading start: \"))\n",
    "depth_end = int(input(\"Depth reading end: \"))\n",
    "depth_step = float(input(\"number of recordings per meter : \")) # 1=1 per m, 0.5=2 per m, 12= every 12 m\n",
    "wellname = input('name of wells: ')\n",
    "filename = input('name of files: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = len(np.arange(depth_start, depth_end, depth_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create las-file from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_gen_input(wellname):\n",
    "    file_arr = np.arange(1,Number_of_files+1,1)\n",
    "    lasfile = ls.LASFile()\n",
    "    from datetime import datetime\n",
    "    lasfile.well.DATE = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    lasfile.well.WELL = wellname\n",
    "    lasfile.params['CR8TOR'] = ls.HeaderItem('CR8TOR', value='Sebastian Aegidius')\n",
    "    lasfile.other = 'LAS file creator'\n",
    "    depth = np.arange(depth_start, depth_end, depth_step)\n",
    "    lasfile.add_curve('DEPTH (m)', depth, unit = 'm')\n",
    "    return lasfile, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasfile, depth = file_gen_input(wellname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(names, info_eq, fgi): # (Name of curve info, equation to produce data, file_gen_input_curve)\n",
    "    data_eq = info_eq\n",
    "    #fake[:3] = np.nan # Set first three values to NaN\n",
    "    fgi.add_curve(names[0], data_eq, unit=names[1], descr=names[2]) # (parameter-name, datavalues, unit, description)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add multiple files, equations and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_files(Number_of_files, wellname = wellname):\n",
    "    list_of_all_wells = []\n",
    "    for i in range(Number_of_files):\n",
    "        sample = file_gen_input(wellname+'_'+str(i))\n",
    "        list_of_all_wells.append(sample[0])\n",
    "    return list_of_all_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [['GR', 'api', 'Gamma Ray'], \n",
    "          ['RD', 'ohm.m', 'Deep Resistivity'], \n",
    "          ['RM', 'ohm.m', 'Medium Resistivity'], \n",
    "          ['RS', 'ohm.m', 'Shallow Resistivity'], \n",
    "          ['NPHI', 'v/v', 'Neutron Porosity'], \n",
    "          ['RHOB', 'g/cm3', 'Density'], \n",
    "          ['DT', 'ms/ft', 'Sonic Velocity'], \n",
    "          ['CALI', 'inch', 'Caliper'], \n",
    "          ['BS', 'inch', 'Bit Size']\n",
    "         ]\n",
    "\n",
    "def eq_realdata(df):\n",
    "    equation = [df[df.columns[0]].values, \n",
    "                df[df.columns[1]].values, \n",
    "                df[df.columns[2]].values,\n",
    "                df[df.columns[3]].values, \n",
    "                df[df.columns[4]].values,\n",
    "                df[df.columns[5]].values,\n",
    "                df[df.columns[6]].values, \n",
    "                df[df.columns[7]].values, \n",
    "                df[df.columns[8]].values]\n",
    "    equation = np.transpose(equation)\n",
    "    equation = equation[0:len_data, :] # Only include equations from our chosen depth-interval\n",
    "    return np.transpose(equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(lasfiles_object, eq_list):\n",
    "    equations = []\n",
    "    for i in range(len(lasfiles_object)): # Make unique equations for each las-file\n",
    "        equations.append(eq_list)\n",
    "    return equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fakedata_multiple_files(fgi, equations, param_list):\n",
    "    for las in range(len(fgi)): # Go through each las-file\n",
    "        random = np.abs(np.random.gamma(shape=3, scale=2, size=len(depth))) # Random-data MUST be inside for-loop!\n",
    "        #random_shift = 10 * np.random.sample() - 5\n",
    "        random_shift = np.random.choice(np.concatenate([(1 + np.random.randn(5)*2), abs(np.random.randn(10)), (8 + np.random.randn(2) * 20)]))\n",
    "        [add_data([param_list[i][0], param_list[i][1], param_list[i][2]], \n",
    "                      equations[las][i]*np.abs(np.random.gamma(shape=3, scale=2, size=len(depth)))\n",
    "                      + equations[las][i]*random_shift, fgi[las]) for i in range(len(param_list))] # add 'random' data to a las-file\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_las(fgi, filename):\n",
    "    [fgi[i].write(filename+'_'+str(i)+'.las', version=2) for i in range(len(fgi))];\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_wells = multiple_files(Number_of_files) # X number of las-files\n",
    "add_fakedata_multiple_files(generated_wells, eq(generated_wells, eq_realdata(df_curves)), params) # Include some data in each file\n",
    "write_to_las(generated_wells, filename) # Update las-files with the new data. TAKES ABOUT 20-30 SEC FOR 100 LAS-FILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seb part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File location and naming\n",
    "\n",
    "def find_files(basedir, extfilter=''):\n",
    "    files_ = []\n",
    "    dirs = [basedir]\n",
    "    while dirs:\n",
    "        for e in os.scandir(dirs.pop()):\n",
    "            if e.is_dir():\n",
    "                dirs.append(e)\n",
    "            else:\n",
    "                if e.name.upper().endswith(extfilter.upper()):\n",
    "                    files_.append(e.path)\n",
    "    return files_\n",
    "\n",
    "def path_leaf(path):\n",
    "    for i in range(len(path)):\n",
    "        path_name, file_name = os.path.split(path)\n",
    "        file_name = file_name.replace('.las','')\n",
    "    return file_name\n",
    "\n",
    "def count_lines(filename):\n",
    "    with open(filename) as f:\n",
    "        return sum(1 for line in f)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    basedir = r\"..\\Documentation\\Well log LAS files\"\n",
    "    start = time.perf_counter()\n",
    "    files = find_files(basedir, '.las')\n",
    "    stop = time.perf_counter()\n",
    "    print('time to scan directory:', stop - start)\n",
    "    print(\"Number of las files   :\", len(files))\n",
    "    print(\"Total lines           :\", sum(map(count_lines, files)))\n",
    "    #print(*files, sep='\\n')\n",
    "    #print()\n",
    "    #file_name = [path_leaf(files[i]) for i in range(len(files))]\n",
    "    #number_of_rows = [count_lines(files[i]) for i in range(len(files))]\n",
    "    #print(*file_name, sep=\"\\n\")\n",
    "    #print(*number_of_rows, sep='     ')\n",
    "    #print()\n",
    "    #print(number_of_rows[64])\n",
    "    #print(file_name[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_las_to_df(filepath_or_lasfile, generated=False):\n",
    "    if (not generated):\n",
    "        las = ls.read(filepath_or_lasfile)\n",
    "        df = las.df()\n",
    "    else:\n",
    "        las = filepath_or_lasfile\n",
    "        df = las.df() \n",
    "    df.reset_index(inplace=True)\n",
    "    df.replace('-1.#IND', np.nan, inplace=True)\n",
    "    df.replace('-1.#IO', np.nan, inplace=True)\n",
    "    [df[x].astype('float64') for x in df.columns] # Convert all parameters to float64\n",
    "    df.replace(-999.25, np.nan, inplace=True)\n",
    "    df.rename(columns={df.columns[0]:'DEPTH'}, inplace=True)\n",
    "    df['WELLNAME'] = las.well.WELL.value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### only to show the list of dataframes\n",
    "list_of_wells_df = [load_las_to_df(files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(list_of_wells_df, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing dataframe to csv file\n",
    "df.to_csv(\"Test_df.csv\")\n",
    "\n",
    "#Writing dataframe to parquet file\n",
    "df.to_parquet('Test_df.parquet', compression = 'gzip', engine='fastparquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dask from las-generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_wells_df = [load_las_to_df(generated_wells[i], generated=True) for i in range(len(generated_wells))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat(generated_wells_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df['Category'] = np.concatenate([['cat'+str(i)]*20000 for i in range(int(len(big_df['WELLNAME'])/20000))]) # len(all_files)/20000 = 10 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerics(df, depthcurvename): # only return the data that is numeric\n",
    "    curve_list=list(df.columns[(df.dtypes.values == np.dtype('float64'))])\n",
    "    curve_list.remove(depthcurvename) # Get rid of depth, so we have 9 remaining logs to plot\n",
    "    return curve_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_curves = get_numerics(big_df, 'DEPTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dummy = dd.from_pandas(big_df, npartitions=len(big_df['Category'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dummy.to_parquet('big_df.parquet', compression = 'gzip', engine='fastparquet')\n",
    "dask_df = dd.read_parquet('big_df.parquet', engine='fastparquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df = dask_df.set_index('WELLNAME').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_color_codes()\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import scale # to scale our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = [files[0], files[4], files[10], files[5]] # Choose three wells from our las-file folder\n",
    "RHOB_names = ['RHOB', 'RHOZ', 'DEN', 'RHOB'] # Store the different density-names (NB: same order as list_files!)\n",
    "NPHI_names = ['NPHI', 'NPHI', 'NPRL', 'NPHI'] # Store the different neutron-names (NB: same order as list_files!)\n",
    "GR_names = ['GR', 'GR', 'GRGC', 'GR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN(df):\n",
    "    for x in df.columns:\n",
    "        if df[x].dtype == 'float64':\n",
    "            len_NaN = len(df[x].isna())\n",
    "            df[x].interpolate(method='linear', axis=0, limit=len_NaN, inplace=True) # Fill NaN values by doing linear interpolation with the preceding and upcoming value\n",
    "            df[x].fillna(method='ffill', inplace=True)\n",
    "            df[x].fillna(method='bfill', inplace=True)\n",
    "            # Replace if any absurdly high or low values:\n",
    "            if abs(np.min(df[x])) > 1000*np.mean(df[x]):\n",
    "                df[x].replace(df[x].min(), np.mean(df[x]))\n",
    "            if abs(np.max(df[x])) > 1000*np.mean(df[x]):\n",
    "                df[x].replace(df[x].max(), np.mean(df[x]))\n",
    "            df[x].fillna(np.mean(df[x]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param1_names and param2_names must be a list of the two parameter names used by the files, in correct order!\n",
    "def init_cluster(list_files, min_depth, max_depth, param1_names, param2_names, param1_realname, param2_realname):\n",
    "    if (str(list_files[0]).lower().endswith('.las')): # if input is realwell\n",
    "        df_and_las = [load_las_to_df(list_files[i]) for i in range(len(list_files))]\n",
    "    else: # if input is generated well\n",
    "        df_and_las = [load_las_to_df(list_files[i], generated=True) for i in range(len(list_files))]\n",
    "    dfs = [df_and_las[i] for i in range(len(list_files))]\n",
    "    \n",
    "    for df in dfs:\n",
    "        if ('DEPTH' not in df.columns):\n",
    "            df.rename(columns={df.columns[0]:'DEPTH'}, inplace=True)\n",
    "    [fill_NaN(i) for i in dfs]\n",
    "    \n",
    "    hold = []\n",
    "    for df, param1_name, param2_name in zip(dfs, param1_names, param2_names):\n",
    "        df = df.loc[df['DEPTH'] > min_depth]\n",
    "        df = df.loc[df['DEPTH'] < max_depth]\n",
    "        df.rename(columns={param1_name:param1_realname}, inplace=True)\n",
    "        df.rename(columns={param2_name:param2_realname}, inplace=True)\n",
    "        hold.append(df)\n",
    "    dfs = hold\n",
    "    return dfs, param1_realname, param2_realname\n",
    "\n",
    "def KMeans_cluster(well_cluster):        \n",
    "    multiple_wells = pd.concat(list(well_cluster[0]))\n",
    "    \n",
    "    new_df = pd.DataFrame({well_cluster[1]:multiple_wells[well_cluster[1]], well_cluster[2]:multiple_wells[well_cluster[2]]})\n",
    "    new_array = np.array(new_df)\n",
    "    \n",
    "    X = scale(new_array)\n",
    "    y = pd.concat([pd.Series(np.zeros(int(len(new_df)/3))), pd.Series(np.ones(int(len(new_df)/3))),\n",
    "               pd.Series(np.ones(int(len(new_df)/3))*2)], ignore_index=True)\n",
    "    \n",
    "    clustering = KMeans(n_clusters=len(list_files))\n",
    "    clustering.fit(X)\n",
    "    clustering.labels_\n",
    "    \n",
    "    palette = sns.color_palette('deep', n_colors=np.unique(clustering.labels_).max() +1) # +1 because it returns index, but we need amount\n",
    "    colors = [palette[x] for x in clustering.labels_]\n",
    "    plt.scatter(x=new_df[well_cluster[1]], y=new_df[well_cluster[2]], color=colors)\n",
    "    plt.xlabel(well_cluster[1])\n",
    "    plt.ylabel(well_cluster[2])\n",
    "    plt.title('Clustering of %d wells' %len(list_files))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_cluster = init_cluster(list_files, 1000, 2000, GR_names, NPHI_names, 'GR', 'NPHI')\n",
    "plot_well_cluster = KMeans_cluster(well_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_ml.datasets\n",
    "import dask_ml.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def interactive_scatter_cluster(big_df, n_clusters=4):\n",
    "    start = time.perf_counter()\n",
    "    @interact(param1 = widgets.Dropdown(\n",
    "                        options = num_curves,\n",
    "                        value = num_curves[0], # Set first well as default value\n",
    "                        description = 'Param 1:',\n",
    "                        disabled = False),\n",
    "              param2 = widgets.Dropdown(\n",
    "                        options = num_curves,\n",
    "                        value = num_curves[1],\n",
    "                        description = 'Param 2:',\n",
    "                        disabled = False),\n",
    "              from_well_nr = widgets.BoundedIntText(\n",
    "                                value=0,\n",
    "                                min=0,\n",
    "                                max=len(big_df['WELLNAME'].unique()),\n",
    "                                step=5,\n",
    "                                description='From well nr:',\n",
    "                                disabled=False\n",
    "                            ),\n",
    "              to_well_nr = widgets.BoundedIntText(\n",
    "                                value=len(big_df['WELLNAME'].unique()),\n",
    "                                min=0,\n",
    "                                max=len(big_df['WELLNAME'].unique()),\n",
    "                                step=5,\n",
    "                                description='To well nr:',\n",
    "                                disabled=False\n",
    "                            )\n",
    "    )\n",
    "    def dask_KMeans(param1, param2, from_well_nr, to_well_nr):\n",
    "        from_well_idx = from_well_nr * len(big_df.loc[big_df['WELLNAME'] == random.choice(big_df['WELLNAME'].unique())])\n",
    "        to_well_idx = to_well_nr * len(big_df.loc[big_df['WELLNAME'] == random.choice(big_df['WELLNAME'].unique())])\n",
    "        \n",
    "        ml_df = pd.DataFrame({param1:big_df.iloc[from_well_idx:to_well_idx][param1], param2:big_df.iloc[from_well_idx:to_well_idx][param2]}) # create a (machine-learning) df containing two parameters\n",
    "        ml_dd = dd.from_pandas(ml_df, npartitions=dask_df.npartitions)\n",
    "        ml_array = ml_dd.to_dask_array(lengths=True) # lengths=True computes chunk size so the array is ready for machine-learning\n",
    "        \n",
    "        km = dask_ml.cluster.KMeans(n_clusters=n_clusters)\n",
    "        km.fit(ml_array)\n",
    "        \n",
    "        ml_dd = ml_dd.assign(x = km.labels_).persist()\n",
    "        scatt = ml_dd.hvplot.scatter(param1, param2, datashade=True, c=km.labels_, cmap='Viridis', \n",
    "                                     title='Clustering of %d wells' %(to_well_nr - from_well_nr))\n",
    "        out = pn.Row(scatt)\n",
    "        return hv.output(out)\n",
    "    stop = time.perf_counter()\n",
    "    print('Time:', stop - start)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_scatter_cluster(big_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section: Wellnames as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wellnames as columns using PANDAS:\n",
    "new_big_df = pd.DataFrame()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for i in range(len(generated_wells)):\n",
    "    param_col = np.concatenate([big_df.loc[big_df['WELLNAME'] == big_df['WELLNAME'].unique()[i]][param[0]].values for param in params])\n",
    "    new_big_df[big_df['WELLNAME'].unique()[i]] = param_col\n",
    "stop = time.perf_counter()\n",
    "print('Timex:', stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include depth and parameter columns\n",
    "depth_col = np.concatenate([big_df.loc[big_df['WELLNAME'] == big_df['WELLNAME'].unique()[0]]['DEPTH (m)'].values for _ in params]) # Same depth for all wells\n",
    "new_big_df['DEPTH (m)'] = depth_col\n",
    "param_col = np.concatenate([[param[0]]*int(len(new_big_df)/len(params)) for param in params])\n",
    "new_big_df['Param'] = param_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get neutron porosity values for 10th column (10th well):\n",
    "new_big_df.loc[new_big_df['Param'] == 'NPHI'][new_big_df.columns[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_big_df.to_parquet('new_big_df.parquet', compression = 'gzip', engine='fastparquet')\n",
    "new_dask_df = dd.read_parquet('new_big_df.parquet', engine='fastparquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare speed of Dask and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_numerics(df, depthcurvename): # only return the data that is numeric\n",
    "    curve_list=list(df.columns[(df.dtypes.values == np.dtype('float64'))])\n",
    "    curve_list.remove(depthcurvename) # Get rid of depth, so we have 9 remaining logs to plot\n",
    "    return curve_list;\n",
    "\n",
    "num_curves = [get_numerics(generated_wells_df[i], 'DEPTH (m)') for i in range(len(generated_wells_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = new_dask_df['Param'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_plot_numerics(log, df, depthname, group=None, alpha=1, colors='blue', height=600, width=300):\n",
    "    aplot = df.hvplot(x=depthname, y=log, by='WELLNAME', groupby=group ,invert=True, flip_yaxis=True, shared_axes=True, alpha=alpha, color=colors,\n",
    "                       height=height, width=width).opts(fontsize={'labels': 16,'xticks': 14, 'yticks': 14})\n",
    "    return aplot;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curve_plot_dask(log, dask_df, depthname, wellname, group=None, alpha=1, colors='blue', height=600, width=300):\n",
    "    plot_df = dd.merge(new_dask_df.loc[new_dask_df['Param'] == log][depthname].compute(), new_dask_df.loc[new_dask_df['Param'] == log][wellname].compute()).T\n",
    "    aplot = plot_df.hvplot(x=depthname, y=wellname, \n",
    "                            groupby=group ,invert=True, flip_yaxis=True, \n",
    "                            shared_axes=False, alpha=alpha, color=colors, height=height, \n",
    "                            width=width).opts(fontsize={'labels': 16,'xticks': 14, 'yticks': 14})\n",
    "    #bplot = hvPlot(pd.DataFrame([new_dask_df.loc[new_dask_df['Param'] == log][depthname].compute(), new_dask_df.loc[new_dask_df['Param'] == log][wellname].compute()]).T,\n",
    "    #              invert=True, flip_yaxis=True, shared_axes=True, alpha=alpha, color=colors, height=height, \n",
    "    #              width=width, fontsize={'labels': 16,'xticks': 14, 'yticks': 14})\n",
    "    return aplot;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT WORKING:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1/4 of the wells:\n",
    "#plotlists = [curve_plot_numerics(x, list_of_wells_df[i], 'DEPTH (m)') for i in range(int(len(list_of_wells_df)/20)) for x in num_curves[i]]\n",
    "start = time.perf_counter()\n",
    "plotlists = [curve_plot_numerics(x, generated_wells_df[0], 'DEPTH (m)') for x in num_curves[0]]\n",
    "stop = time.perf_counter()\n",
    "print('Time Pandas:', stop-start)\n",
    "\n",
    "start = time.perf_counter()\n",
    "plotlists_dask = [curve_plot_dask(log, new_dask_df, 'DEPTH (m)', 'DaskSpeed2_10') for log in parameters]\n",
    "stop = time.perf_counter()\n",
    "print('Time Dask:', stop-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
