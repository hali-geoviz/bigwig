{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vetle + Seb code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import lasio as ls\n",
    "#import selenium\n",
    "#import phantomjs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "#import pyarrow.parquet as pq\n",
    "import fastparquet\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "import hvplot.dask\n",
    "from hvplot import hvPlot\n",
    "import holoviews as hv\n",
    "from holoviews import opts, streams\n",
    "from holoviews.plotting.links import DataLink\n",
    "hv.extension('bokeh', logo=None)\n",
    "from IPython.display import display, HTML\n",
    "from bokeh.models.tools import LassoSelectTool, BoxSelectTool  # tools=['hover']\n",
    "box_select = BoxSelectTool()\n",
    "lasso_select = LassoSelectTool()\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "client_secret = \"Bw2w*31.CmbIiP.i2EILQ=HN@xr]yeu?\"\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 80%; }\n",
    "    div#maintoolbar-container { width: 80%; }\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datashader as ds\n",
    "from holoviews.streams import Selection1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../Documentation/Well log LAS files/WKF W8A LWD VISION RM 0_5m Drilling.LAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def las_to_df(filepath):\n",
    "    las = ls.read(filepath)\n",
    "    curve_df = las.df()\n",
    "    curve_df = curve_df.replace(-999.25,np.nan)\n",
    "    curve_df.reset_index(inplace=True)\n",
    "    curve_df.rename(columns={curve_df.columns[0]:'Depth'}, inplace=True)\n",
    "    las.curves[0].mnemonic = 'Depth'\n",
    "    return curve_df, las"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_realwell, las_realwell = las_to_df(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df only containing relevant curves (9) within a start and stop depth:\n",
    "def get_curves(df, curvenames, start_depth, stop_depth):\n",
    "    df_new = df.loc[start_depth<=df['Depth']].loc[df['Depth']<stop_depth][curvenames]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_curves = ['Depth', 'GR_ARC', 'A16H', 'A28H', 'A40H', 'TNPH', 'ROP5_RM', 'RHOB', 'DCAV'] # Relevant curve names for the chosen well\n",
    "\n",
    "start_depth = df_realwell['Depth'].iloc[0]\n",
    "end_depth = df_realwell['Depth'].iloc[-1]\n",
    "step_depth = df_realwell['Depth'].iloc[1] - df_realwell['Depth'].iloc[0]\n",
    "\n",
    "df_curves = get_curves(df_realwell, all_curves, start_depth, end_depth) # New df containing specific curves and start and stop depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_files = int(input(\"Number of files to generate: \"))\n",
    "depth_start = int(input(\"Depth reading start: \"))\n",
    "depth_end = int(input(\"Depth reading end: \"))\n",
    "depth_step = float(input(\"number of recordings per meter : \")) # 1=1 per m, 0.5=2 per m, 12= every 12 m\n",
    "wellname = input('name of wells: ')\n",
    "filename = input('name of files: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = len(np.arange(depth_start, depth_end, depth_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create las-file from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_gen_input(wellname):\n",
    "    file_arr = np.arange(1,Number_of_files+1,1)\n",
    "    lasfile = ls.LASFile()\n",
    "    from datetime import datetime\n",
    "    lasfile.well.DATE = datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    lasfile.well.WELL = wellname\n",
    "    lasfile.params['CR8TOR'] = ls.HeaderItem('CR8TOR', value='Sebastian Aegidius')\n",
    "    lasfile.other = 'LAS file creator'\n",
    "    depth = np.arange(depth_start, depth_end, depth_step)\n",
    "    lasfile.add_curve('DEPTH (m)', depth, unit = 'm')\n",
    "    return lasfile, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasfile, depth = file_gen_input(wellname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(names, info_eq, fgi): # (Name of curve info, equation to produce data, file_gen_input_curve)\n",
    "    data_eq = info_eq\n",
    "    #fake[:3] = np.nan # Set first three values to NaN\n",
    "    fgi.add_curve(names[0], data_eq, unit=names[1], descr=names[2]) # (parameter-name, datavalues, unit, description)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add multiple files, equations and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_files(Number_of_files, wellname = wellname):\n",
    "    list_of_all_wells = []\n",
    "    for i in range(Number_of_files):\n",
    "        sample = file_gen_input(wellname+'_'+str(i))\n",
    "        list_of_all_wells.append(sample[0])\n",
    "    return list_of_all_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [['GR', 'api', 'Gamma Ray'], \n",
    "          ['RD', 'ohm.m', 'Deep Resistivity'], \n",
    "          ['RM', 'ohm.m', 'Medium Resistivity'], \n",
    "          ['RS', 'ohm.m', 'Shallow Resistivity'], \n",
    "          ['NPHI', 'v/v', 'Neutron Porosity'], \n",
    "          ['RHOB', 'g/cm3', 'Density'], \n",
    "          ['DT', 'ms/ft', 'Sonic Velocity'], \n",
    "          ['CALI', 'inch', 'Caliper'], \n",
    "          ['BS', 'inch', 'Bit Size']\n",
    "         ]\n",
    "\n",
    "def eq_realdata(df):\n",
    "    equation = [df[df.columns[0]].values, \n",
    "                df[df.columns[1]].values, \n",
    "                df[df.columns[2]].values,\n",
    "                df[df.columns[3]].values, \n",
    "                df[df.columns[4]].values,\n",
    "                df[df.columns[5]].values,\n",
    "                df[df.columns[6]].values, \n",
    "                df[df.columns[7]].values, \n",
    "                df[df.columns[8]].values]\n",
    "    equation = np.transpose(equation)\n",
    "    equation = equation[0:len_data, :] # Only include equations from our chosen depth-interval\n",
    "    return np.transpose(equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq(lasfiles_object, eq_list):\n",
    "    equations = []\n",
    "    for i in range(len(lasfiles_object)): # Make unique equations for each las-file\n",
    "        equations.append(eq_list)\n",
    "    return equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fakedata_multiple_files(fgi, equations, param_list):\n",
    "    for las in range(len(fgi)): # Go through each las-file\n",
    "        random = np.abs(np.random.gamma(shape=3, scale=2, size=len(depth))) # Random-data MUST be inside for-loop!\n",
    "        #random_shift = 10 * np.random.sample() - 5\n",
    "        random_shift = np.random.choice(np.concatenate([(1 + np.random.randn(5)*2), abs(np.random.randn(10)), (8 + np.random.randn(2) * 20)]))\n",
    "        [add_data([param_list[i][0], param_list[i][1], param_list[i][2]], \n",
    "                      equations[las][i]*np.abs(np.random.gamma(shape=3, scale=2, size=len(depth)))\n",
    "                      + equations[las][i]*random_shift, fgi[las]) for i in range(len(param_list))] # add 'random' data to a las-file\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_las(fgi, filename):\n",
    "    [fgi[i].write(filename+'_'+str(i)+'.las', version=2) for i in range(len(fgi))];\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_wells = multiple_files(Number_of_files) # X number of las-files\n",
    "add_fakedata_multiple_files(generated_wells, eq(generated_wells, eq_realdata(df_curves)), params) # Include some data in each file\n",
    "write_to_las(generated_wells, filename) # Update las-files with the new data. TAKES ABOUT 20-30 SEC FOR 100 LAS-FILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seb part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File location and naming\n",
    "\n",
    "def find_files(basedir, extfilter=''):\n",
    "    files_ = []\n",
    "    dirs = [basedir]\n",
    "    while dirs:\n",
    "        for e in os.scandir(dirs.pop()):\n",
    "            if e.is_dir():\n",
    "                dirs.append(e)\n",
    "            else:\n",
    "                if e.name.upper().endswith(extfilter.upper()):\n",
    "                    files_.append(e.path)\n",
    "    return files_\n",
    "\n",
    "def path_leaf(path):\n",
    "    for i in range(len(path)):\n",
    "        path_name, file_name = os.path.split(path)\n",
    "        file_name = file_name.replace('.las','')\n",
    "    return file_name\n",
    "\n",
    "def count_lines(filename):\n",
    "    with open(filename) as f:\n",
    "        return sum(1 for line in f)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    basedir = r\"..\\Documentation\\Well log LAS files\"\n",
    "    start = time.perf_counter()\n",
    "    files = find_files(basedir, '.las')\n",
    "    stop = time.perf_counter()\n",
    "    print('time to scan directory:', stop - start)\n",
    "    print(\"Number of las files   :\", len(files))\n",
    "    print(\"Total lines           :\", sum(map(count_lines, files)))\n",
    "    #print(*files, sep='\\n')\n",
    "    #print()\n",
    "    #file_name = [path_leaf(files[i]) for i in range(len(files))]\n",
    "    #number_of_rows = [count_lines(files[i]) for i in range(len(files))]\n",
    "    #print(*file_name, sep=\"\\n\")\n",
    "    #print(*number_of_rows, sep='     ')\n",
    "    #print()\n",
    "    #print(number_of_rows[64])\n",
    "    #print(file_name[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_las_to_df(filepath_or_lasfile, generated=False):\n",
    "    if (not generated):\n",
    "        las = ls.read(filepath_or_lasfile)\n",
    "        df = las.df()\n",
    "    else:\n",
    "        las = filepath_or_lasfile\n",
    "        df = las.df() \n",
    "    df.reset_index(inplace=True)\n",
    "    df.replace('-1.#IND', np.nan, inplace=True)\n",
    "    df.replace('-1.#IO', np.nan, inplace=True)\n",
    "    \n",
    "    # Covnert objects to numerics:\n",
    "    cols = df.columns[df.dtypes.eq(object)]\n",
    "    df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    df.replace(-999.25, np.nan, inplace=True)\n",
    "    df.rename(columns={df.columns[0]:'DEPTH'}, inplace=True)\n",
    "    df['WELLNAME'] = las.well.WELL.value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### only to show the list of dataframes\n",
    "list_of_wells_df = [load_las_to_df(files[i]) for i in range(len(files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(list_of_wells_df, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing dataframe to csv file\n",
    "df.to_csv(\"Test_df.csv\")\n",
    "\n",
    "#Writing dataframe to parquet file\n",
    "df.to_parquet('Test_df.parquet', compression = 'gzip', engine='fastparquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some categories, and create dask from las-generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_wells_df = [load_las_to_df(generated_wells[i], generated=True) for i in range(len(generated_wells))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat(generated_wells_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_cat = 10\n",
    "big_df['Category'] = np.concatenate([['cat'+str(i)]*int((len(big_df['WELLNAME'])/nr_cat)) for i in range(int(len(big_df['WELLNAME'])/int(len(big_df['WELLNAME'])/nr_cat)))]) # len(all_files)/20000 = 10 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerics(df, depthcurvename): # only return the data that is numeric\n",
    "    curve_list=list(df.columns[(df.dtypes.values == np.dtype('float64'))])\n",
    "    curve_list.remove(depthcurvename) # Get rid of depth, so we have 9 remaining logs to plot\n",
    "    return curve_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_curves = get_numerics(big_df, 'DEPTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lat_long(fgi, fgi_df):\n",
    "    for i in range(len(fgi)):\n",
    "        fgi[i].well['LATITUDE'] = ls.HeaderItem(mnemonic='LAT', unit='DEG', value=float('{:.8f}'.format((2 * np.random.sample() - 1)*90)), descr='Latitude')\n",
    "        fgi[i].well['LONGITUDE'] = ls.HeaderItem(mnemonic='LONG', unit='DEG', value=float('{:.8f}'.format((2 * np.random.sample() - 1)*180)), descr='Longitude')\n",
    "        start_idx = int(len(fgi_df)*i/len(fgi))\n",
    "        stop_idx = int(len(fgi_df)*(i+1)/len(fgi))\n",
    "        fgi_df.loc[start_idx:stop_idx, 'LATITUDE'] = fgi[i].well['LAT'].value # Sets specific latitude for each well\n",
    "        fgi_df.loc[start_idx:stop_idx, 'LONGITUDE'] = fgi[i].well['LONG'].value # Sets specific longitude for each well\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long(generated_wells, big_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df['Formation'] = ''\n",
    "conditionFens = (big_df['DEPTH'] >= 1200) & (big_df['DEPTH'] < 1500)\n",
    "conditionSogn = (big_df['DEPTH'] >= 1500) & (big_df['DEPTH'] < 1800)\n",
    "big_df['Formation'] = big_df['Formation'].mask(conditionFens, 'Fensfjord')\n",
    "big_df['Formation'] = big_df['Formation'].mask(conditionSogn, 'Sognefjord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation_color = dict(pd.Series([{form: color for form, color in zip(big_df['Formation'].unique(), [0, 1, 2])}])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df['Formation_col'] = 0\n",
    "for form, col in zip(formation_color.keys(), formation_color.values()):\n",
    "    condition_form = big_df['Formation'] == form\n",
    "    big_df['Formation_col'] = big_df['Formation_col'].mask(condition_form, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dummy = dd.from_pandas(big_df, npartitions=len(big_df['Category'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dummy.to_parquet('big_df.parquet', compression = 'gzip', engine='fastparquet')\n",
    "dask_df = dd.read_parquet('big_df.parquet', engine='fastparquet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_df = dask_df.set_index('WELLNAME').persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot + Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from holoviews.operation import histogram\n",
    "\n",
    "@interact(paramX = widgets.Dropdown(\n",
    "                    options = num_curves,\n",
    "                    value = num_curves[0], # Set first well as default value\n",
    "                    description = 'Param x:',\n",
    "                    disabled = False),\n",
    "          paramY = widgets.Dropdown(\n",
    "                    options = num_curves,\n",
    "                    value = num_curves[5], # Set second wel as default value\n",
    "                    description = 'Param y:',\n",
    "                    disabled = False),\n",
    "          param_color = widgets.Dropdown(\n",
    "                    options = num_curves + ['DEPTH'] + ['Form_color'] + ['Negative'], # all num_curves[i] contains same 9 parameters\n",
    "                    value = None,\n",
    "                    description = 'Color by:',\n",
    "                    disabled = False),\n",
    "          from_wellA = widgets.BoundedIntText(\n",
    "                    value=1,\n",
    "                    min=1,\n",
    "                    max=len(dask_df.index.value_counts().compute()),\n",
    "                    step=5,\n",
    "                    description='A: From well nr:',\n",
    "                    disabled=False\n",
    "                ),\n",
    "          to_wellA = widgets.BoundedIntText(\n",
    "                    value=np.floor(len(dask_df.index.value_counts().compute())/2),\n",
    "                    min=1,\n",
    "                    max=len(dask_df.index.value_counts().compute()),\n",
    "                    step=5,\n",
    "                    description='A: To well nr:',\n",
    "                    disabled=False\n",
    "                ),\n",
    "          from_wellB = widgets.BoundedIntText(\n",
    "                    value=np.floor(len(dask_df.index.value_counts().compute())/2) + 1,\n",
    "                    min=1,\n",
    "                    max=len(dask_df.index.value_counts().compute()),\n",
    "                    step=5,\n",
    "                    description='B: From well nr:',\n",
    "                    disabled=False\n",
    "                ),\n",
    "          to_wellB = widgets.BoundedIntText(\n",
    "                    value=len(dask_df.index.value_counts().compute()),\n",
    "                    min=1,\n",
    "                    max=len(dask_df.index.value_counts().compute()),\n",
    "                    step=5,\n",
    "                    description='B: To well nr:',\n",
    "                    disabled=False\n",
    "                ),\n",
    "          scatter_overlay = widgets.Checkbox(\n",
    "                    value=False,\n",
    "                    description='Overlay scatterplots',\n",
    "                    disabled=False,\n",
    "                    indent=True),\n",
    "          datashade = widgets.Checkbox(\n",
    "                    value=True,\n",
    "                    description='Datashade',\n",
    "                    disabled=False,\n",
    "                    indent=True),\n",
    "          random_data = widgets.ToggleButton(\n",
    "                    value=False,\n",
    "                    description='Random data',\n",
    "                    disabled=False,\n",
    "                    icon='check'),\n",
    ")\n",
    "def curve_plot_dask(paramX, paramY, param_color, from_wellA, to_wellA, from_wellB, to_wellB, scatter_overlay, datashade, random_data):\n",
    "    if (random_data):\n",
    "        paramX = np.random.choice(num_curves)\n",
    "        paramY = np.random.choice(num_curves)\n",
    "        param_color = np.random.choice(num_curves+['DEPTH']+['Blue']+['Negative'])\n",
    "        from_wellA = np.random.choice(len(dask_df.index.value_counts().compute()))\n",
    "        to_wellA = np.random.choice(np.arange(from_wellA, len(dask_df.index.value_counts().compute())))\n",
    "        from_wellB = np.random.choice(len(dask_df.index.value_counts().compute()))\n",
    "        to_wellB = np.random.choice(np.arange(from_wellB, len(dask_df.index.value_counts().compute())))\n",
    "    \n",
    "    from_wellA = from_wellA - 1 # Account for index\n",
    "    to_wellA = to_wellA - 1\n",
    "    from_wellB = from_wellB - 1\n",
    "    to_wellB = to_wellB -1\n",
    "    \n",
    "    A_from_idx = big_df.loc[big_df['WELLNAME'] == wellname+'_'+str(from_wellA)].index[0]\n",
    "    A_to_idx = big_df.loc[big_df['WELLNAME'] == wellname+'_'+str(to_wellA)].index[-1]\n",
    "    B_from_idx = big_df.loc[big_df['WELLNAME'] == wellname+'_'+str(from_wellB)].index[0]\n",
    "    B_to_idx = big_df.loc[big_df['WELLNAME'] == wellname+'_'+str(to_wellB)].index[-1]\n",
    "    \n",
    "    big_df['Negative'] = 0\n",
    "    condition_neg = (big_df[paramX] < 0) | (big_df[paramY] < 0) \n",
    "    big_df['Negative'] = big_df['Negative'].mask(condition_neg, 1)\n",
    "    \n",
    "    logx = logy = False\n",
    "    if (paramX == 'RS' or paramX == 'RM' or paramX == 'RD'): logx = True \n",
    "    if (paramY == 'RS' or paramY == 'RM' or paramY == 'RD'): logy = True\n",
    "    cmapA = 'Winter'\n",
    "    cmapB = 'Inferno'\n",
    "    if (scatter_overlay):\n",
    "        param_color = None\n",
    "    aplot = big_df.iloc[A_from_idx:A_to_idx].hvplot.scatter(x=paramX, y=paramY, logx=logx, logy=logy,\n",
    "                                                color=param_color, datashade=datashade, cmap=cmapA, label='Group A color: '+cmapA,\n",
    "                                                height=400, width=600, title=f'A: Correlation of {paramX} and {paramY} for well {from_wellA+1} to well {to_wellA+1}')\n",
    "    bplot = big_df.iloc[B_from_idx:B_to_idx].hvplot.scatter(x=paramX, y=paramY, logx=logx, logy=logy,\n",
    "                                                color=param_color, datashade=datashade, cmap=cmapB, label='Group B color: '+cmapB,\n",
    "                                                height=400, width=600, title=f'B: Correlation of {paramX} and {paramY} for well {from_wellB+1} to well {to_wellB+1}') \n",
    "    \n",
    "    histograms = [ hv.NdOverlay({str(big_df.iloc[i].WELLNAME)+' to '+str(big_df.iloc[j].WELLNAME): \n",
    "                             big_df.iloc[i:j].hvplot.hist(y=param, logx=logx, logy=logy, invert=invert, bin_range=(min(dask_df[param]), max(dask_df[param])), \n",
    "                             bins=50) for i, j in zip([A_from_idx, B_from_idx], [A_to_idx, B_to_idx])}).opts(\n",
    "                             'Histogram', height=400, width=600, alpha=0.6, muted_fill_alpha=0.15) for param, invert, logx, logy in zip([paramX, paramY], [False, True], [logx, False], [False, logy]) ]\n",
    "    \n",
    "    if (scatter_overlay): return pn.Column(pn.Row((aplot * bplot).opts(title=f'Correlation of {paramX} and {paramY} for A: {to_wellA - from_wellA} wells, and B: {to_wellB - from_wellB} wells')), pn.Row(histograms[0], histograms[1]))\n",
    "    else: return pn.Column(pn.Row(aplot, bplot), pn.Row(histograms[0], histograms[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(paramX = widgets.Dropdown(\n",
    "                    options = num_curves,\n",
    "                    value = num_curves[0], # Set first well as default value\n",
    "                    description = 'Param x:',\n",
    "                    disabled = False),\n",
    "          paramY = widgets.Dropdown(\n",
    "                    options = num_curves,\n",
    "                    value = num_curves[1], # Set second wel as default value\n",
    "                    description = 'Param y:',\n",
    "                    disabled = False),\n",
    "          param_color = widgets.Dropdown(\n",
    "                    options = num_curves + ['DEPTH'] + ['Blue'], # all num_curves[i] contains same 9 parameters\n",
    "                    value = 'Blue',\n",
    "                    description = 'Color by:',\n",
    "                    disabled = False),\n",
    "          from_wellA = widgets.BoundedIntText(\n",
    "                    value=0,\n",
    "                    min=0,\n",
    "                    max=len(dask_df.index.value_counts().compute()),\n",
    "                    step=5,\n",
    "                    description='A: From well nr:',\n",
    "                    disabled=False\n",
    "                ),\n",
    "          to_wellA = widgets.BoundedIntText(\n",
    "                    value=np.floor(len(dask_df.index.value_counts().compute())/2) - 1, # -1 to account for index\n",
    "                    min=0,\n",
    "                    max=len(dask_df.index.value_counts().compute()) - 1,\n",
    "                    step=5,\n",
    "                    description='A: To well nr:',\n",
    "                    disabled=False\n",
    "                ),\n",
    "          to_wellB = widgets.BoundedIntText(\n",
    "                    value=len(dask_df.index.value_counts().compute()) - 1,\n",
    "                    min=0,\n",
    "                    max=len(dask_df.index.value_counts().compute()) - 1,\n",
    "                    step=5,\n",
    "                    description='B: To well nr:',\n",
    "                    disabled=False\n",
    "                ),\n",
    "          datashade = widgets.Checkbox(\n",
    "                    value=True,\n",
    "                    description='Datashade',\n",
    "                    disabled=False,\n",
    "                    indent=True),\n",
    ")\n",
    "def curve_plot_dask(paramX, paramY, param_color, from_wellA, to_wellA, to_wellB, datashade):\n",
    "    aplot = dask_df.loc[wellname+'_'+str(from_wellA):wellname+'_'+str(to_wellA)].hvplot.scatter(x=paramX, y=paramY, \n",
    "                                                color=param_color, datashade=datashade, cmap='Hot',\n",
    "                                                height=400, width=600)\n",
    "    bplot = dask_df.loc[wellname+'_'+str(to_wellA + 1):wellname+'_'+str(to_wellB)].hvplot.scatter(x=paramX, y=paramY, \n",
    "                                                color=param_color, datashade=datashade, cmap='Winter',\n",
    "                                                height=400, width=600)\n",
    "    #histX = dask_df.hvplot.hist(y=paramX, bin_range=(min(dask_df[paramX]), max(dask_df[paramX])), bins=50)\n",
    "    #histY = dask_df.hvplot.hist(y=paramY, bin_range=(min(dask_df[paramY]), max(dask_df[paramY])), bins=50)\n",
    "    histX, histY = (histogram(bplot, bin_range=(min(dask_df[paramX]), max(dask_df[paramX])), dimension=dim) *\n",
    "                histogram(aplot,  bin_range=(min(dask_df[paramY]), max(dask_df[paramY])), dimension=dim) \n",
    "                for dim in [paramX, paramY])\n",
    "    #return aplot.hist(dimension=[paramX, paramY])\n",
    "    #hv.output(aplot)\n",
    "    composition = (bplot * aplot) << histY.opts(width=150, xlim=(min(dask_df[paramY])/10, max(dask_df[paramY])/10)) << histX.opts(height=150, xlim=(min(dask_df[paramX])/10, max(dask_df[paramX])/10))\n",
    "    composition.opts(opts.Histogram(alpha=0.3))\n",
    "    #return pn.Column(pn.Row(aplot, bplot), pn.Row(histX, histY))\n",
    "    return composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_color_codes()\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import scale # to scale our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = [files[0], files[4], files[10], files[5]] # Choose three wells from our las-file folder\n",
    "RHOB_names = ['RHOB', 'RHOZ', 'DEN', 'RHOB'] # Store the different density-names (NB: same order as list_files!)\n",
    "NPHI_names = ['NPHI', 'NPHI', 'NPRL', 'NPHI'] # Store the different neutron-names (NB: same order as list_files!)\n",
    "GR_names = ['GR', 'GR', 'GRGC', 'GR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_NaN(df):\n",
    "    for x in df.columns:\n",
    "        if df[x].dtype == 'float64':\n",
    "            len_NaN = len(df[x].isna())\n",
    "            df[x].interpolate(method='linear', axis=0, limit=len_NaN, inplace=True) # Fill NaN values by doing linear interpolation with the preceding and upcoming value\n",
    "            df[x].fillna(method='ffill', inplace=True)\n",
    "            df[x].fillna(method='bfill', inplace=True)\n",
    "            # Replace if any absurdly high or low values:\n",
    "            if abs(np.min(df[x])) > 1000*np.mean(df[x]):\n",
    "                df[x].replace(df[x].min(), np.mean(df[x]))\n",
    "            if abs(np.max(df[x])) > 1000*np.mean(df[x]):\n",
    "                df[x].replace(df[x].max(), np.mean(df[x]))\n",
    "            df[x].fillna(np.mean(df[x]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param1_names and param2_names must be a list of the two parameter names used by the files, in correct order!\n",
    "def init_cluster(list_files, min_depth, max_depth, param1_names, param2_names, param1_realname, param2_realname):\n",
    "    if (str(list_files[0]).lower().endswith('.las')): # if input is realwell\n",
    "        df_and_las = [load_las_to_df(list_files[i]) for i in range(len(list_files))]\n",
    "    else: # if input is generated well\n",
    "        df_and_las = [load_las_to_df(list_files[i], generated=True) for i in range(len(list_files))]\n",
    "    dfs = [df_and_las[i] for i in range(len(list_files))]\n",
    "    \n",
    "    for df in dfs:\n",
    "        if ('DEPTH' not in df.columns):\n",
    "            df.rename(columns={df.columns[0]:'DEPTH'}, inplace=True)\n",
    "    [fill_NaN(i) for i in dfs]\n",
    "    \n",
    "    hold = []\n",
    "    for df, param1_name, param2_name in zip(dfs, param1_names, param2_names):\n",
    "        df = df.loc[df['DEPTH'] > min_depth]\n",
    "        df = df.loc[df['DEPTH'] < max_depth]\n",
    "        df.rename(columns={param1_name:param1_realname}, inplace=True)\n",
    "        df.rename(columns={param2_name:param2_realname}, inplace=True)\n",
    "        hold.append(df)\n",
    "    dfs = hold\n",
    "    return dfs, param1_realname, param2_realname\n",
    "\n",
    "def KMeans_cluster(well_cluster):        \n",
    "    multiple_wells = pd.concat(list(well_cluster[0]))\n",
    "    \n",
    "    new_df = pd.DataFrame({well_cluster[1]:multiple_wells[well_cluster[1]], well_cluster[2]:multiple_wells[well_cluster[2]]})\n",
    "    new_array = np.array(new_df)\n",
    "    \n",
    "    X = scale(new_array)\n",
    "    y = pd.concat([pd.Series(np.zeros(int(len(new_df)/3))), pd.Series(np.ones(int(len(new_df)/3))),\n",
    "               pd.Series(np.ones(int(len(new_df)/3))*2)], ignore_index=True)\n",
    "    \n",
    "    clustering = KMeans(n_clusters=len(list_files))\n",
    "    clustering.fit(X)\n",
    "    clustering.labels_\n",
    "    \n",
    "    palette = sns.color_palette('deep', n_colors=np.unique(clustering.labels_).max() +1) # +1 because it returns index, but we need amount\n",
    "    colors = [palette[x] for x in clustering.labels_]\n",
    "    plt.scatter(x=new_df[well_cluster[1]], y=new_df[well_cluster[2]], color=colors)\n",
    "    plt.xlabel(well_cluster[1])\n",
    "    plt.ylabel(well_cluster[2])\n",
    "    plt.title('Clustering of %d wells' %len(list_files))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_cluster = init_cluster(list_files, 1000, 2000, GR_names, NPHI_names, 'GR', 'NPHI')\n",
    "plot_well_cluster = KMeans_cluster(well_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_ml.datasets\n",
    "import dask_ml.cluster\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def interactive_scatter_cluster(big_df, n_clusters=4):\n",
    "    start = time.perf_counter()\n",
    "    @interact(param1 = widgets.Dropdown(\n",
    "                        options = num_curves,\n",
    "                        value = num_curves[0], # Set first well as default value\n",
    "                        description = 'Param 1:',\n",
    "                        disabled = False),\n",
    "              param2 = widgets.Dropdown(\n",
    "                        options = num_curves,\n",
    "                        value = num_curves[1],\n",
    "                        description = 'Param 2:',\n",
    "                        disabled = False),\n",
    "              from_well_nr = widgets.BoundedIntText(\n",
    "                                value=0,\n",
    "                                min=0,\n",
    "                                max=len(big_df['WELLNAME'].unique()),\n",
    "                                step=5,\n",
    "                                description='From well nr:',\n",
    "                                disabled=False\n",
    "                            ),\n",
    "              to_well_nr = widgets.BoundedIntText(\n",
    "                                value=len(big_df['WELLNAME'].unique()),\n",
    "                                min=0,\n",
    "                                max=len(big_df['WELLNAME'].unique()),\n",
    "                                step=5,\n",
    "                                description='To well nr:',\n",
    "                                disabled=False\n",
    "                            )\n",
    "    )\n",
    "    def dask_KMeans(param1, param2, from_well_nr, to_well_nr):\n",
    "        from_well_idx = from_well_nr * len(big_df.loc[big_df['WELLNAME'] == random.choice(big_df['WELLNAME'].unique())])\n",
    "        to_well_idx = to_well_nr * len(big_df.loc[big_df['WELLNAME'] == random.choice(big_df['WELLNAME'].unique())])\n",
    "        \n",
    "        ml_df = pd.DataFrame({param1:big_df.iloc[from_well_idx:to_well_idx][param1], param2:big_df.iloc[from_well_idx:to_well_idx][param2]}) # create a (machine-learning) df containing two parameters\n",
    "        ml_dd = dd.from_pandas(ml_df, npartitions=dask_df.npartitions)\n",
    "        ml_array = ml_dd.to_dask_array(lengths=True) # lengths=True computes chunk size so the array is ready for machine-learning\n",
    "        \n",
    "        km = dask_ml.cluster.KMeans(n_clusters=n_clusters)\n",
    "        km.fit(ml_array)\n",
    "        \n",
    "        ml_dd = ml_dd.assign(x = km.labels_).persist()\n",
    "        scatt = ml_dd.hvplot.scatter(param1, param2, datashade=True, c=km.labels_, cmap='Viridis', \n",
    "                                     title='Clustering of %d wells' %(to_well_nr - from_well_nr))\n",
    "        out = pn.Row(scatt)\n",
    "        return hv.output(out)\n",
    "    stop = time.perf_counter()\n",
    "    print('Time:', stop - start)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_scatter_cluster(big_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import colors as mcolors\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def cluster_DBSCAN(big_df, param1, param2, eps=0.2, min_samples=10, from_well_nr=0, to_well_nr=10):\n",
    "    from_well_idx = from_well_nr * len(big_df.loc[big_df['WELLNAME'] == random.choice(big_df['WELLNAME'].unique())])\n",
    "    to_well_idx = to_well_nr * len(big_df.loc[big_df['WELLNAME'] == random.choice(big_df['WELLNAME'].unique())])\n",
    "    \n",
    "    ml_df = pd.DataFrame({param1:big_df.iloc[from_well_idx:to_well_idx][param1], param2:big_df.iloc[from_well_idx:to_well_idx][param2]}) # create a (machine-learning) df containing two parameters\n",
    "    ml_dd = dd.from_pandas(ml_df, npartitions=dask_df.npartitions)\n",
    "    ml_array = ml_dd.to_dask_array(lengths=True)\n",
    "    \n",
    "    X = StandardScaler().fit_transform(ml_array)\n",
    "    print(X)\n",
    "    # Compute DBSCAN\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    \n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % silhouette_score(X, labels))\n",
    "    \n",
    "    # Plot results:\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Accent(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    #colors = cm.nipy_spectral(labels.astype(float) / len(unique_labels))\n",
    "    colors = [mcolors.rgb2hex(c) for c in colors]\n",
    "    all_scatt = []\n",
    "    #hv_image = hv.Overlay([])\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for outliers.\n",
    "            col = mcolors.rgb2hex([0, 0, 0, 1])\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "        part_of_core_or_not = [core_samples_mask, ~core_samples_mask]\n",
    "        size = 10\n",
    "        group_scatt = []\n",
    "        \n",
    "        for in_core_check in part_of_core_or_not:\n",
    "            xy = ml_array[class_member_mask & in_core_check] # Clear part of cluster\n",
    "            if (len(xy) > 0):\n",
    "                ddf = dd.concat([dd.from_dask_array(i) for i in [xy[:,0], xy[:,1]]]) # !!!!!!!\n",
    "                ddfA = da.from_array(ddf.reset_index()[0].compute()[0:int(len(ddf)/2)])\n",
    "                ddfB = da.from_array(ddf.reset_index()[0].compute()[int(len(ddf)/2):])\n",
    "                ddfA = list(ddfA.compute())\n",
    "                ddfB = list(ddfB.compute())\n",
    "                testy = pd.DataFrame({'a':ddfA, 'b':ddfB, 'c':[str(col)]*len(ddfA)})\n",
    "                #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                #         markeredgecolor='k', markersize=14)\n",
    "                group_scatt.append(testy.hvplot.scatter('a', 'b', c='c', s=size).opts(xlabel=param1, ylabel=param2, show_legend=True, colorbar=True))\n",
    "            else:\n",
    "                group_scatt.append(hv.Overlay([]))\n",
    "            size*=0.25\n",
    "        [all_scatt.append(group_scatt[i]) for i in range(len(group_scatt))]\n",
    "    print(all_scatt)\n",
    "    img = hv.Overlay(all_scatt).opts(toolbar='right')\n",
    "    hv.output(img)\n",
    "    #plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    #plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_DBSCAN(big_df, 'GR', 'RD', from_well_nr=60, to_well_nr=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datashader.geo import lnglat_to_meters\n",
    "from holoviews.element import tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = lnglat_to_meters(dask_df['LONGITUDE'], dask_df['LATITUDE'])\n",
    "dask_df = dask_df.assign(x=x, y=y).persist()\n",
    "big_df['x'] = x\n",
    "big_df['y'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame()\n",
    "for param in range(len(num_curves)):\n",
    "    param_curve = dask_df.iloc[:, param].compute()[::len(dask_df.loc['MapTest_0'])] # One of each param-value for each file\n",
    "    param_df[param_curve.name] = param_curve.values\n",
    "param_df['WELLNAME'] = dask_df.iloc[:,param].compute()[::len(dask_df.loc['MapTest_0'])].index.values\n",
    "param_df['LATITUDE'] = [float(big_df.loc[big_df['WELLNAME'] == wellname]['LATITUDE'].unique()) for wellname in param_df['WELLNAME']]\n",
    "param_df['LONGITUDE'] = [float(big_df.loc[big_df['WELLNAME'] == wellname]['LONGITUDE'].unique()) for wellname in param_df['WELLNAME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big_df.loc[big_df['WELLNAME'] == 'MapTest_3'].loc[big_df['DEPTH'] >1800].loc[big_df['DEPTH'] <2100]['Formation']\n",
    "param_df['Formation'] = [np.random.choice(big_df.loc[big_df['DEPTH'] == int(param_df.loc[param_df['WELLNAME'] == wellname]['DEPTH'].values)]['Formation']) for wellname in param_df['WELLNAME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df['x'], param_df['y'] = lnglat_to_meters(param_df['LONGITUDE'], param_df['LATITUDE'])\n",
    "param_df['RHOB_col'] = param_df['RHOB'].astype('int64') # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df2 = param_df.copy()\n",
    "param_df2['GR'] = param_df['GR'].where(param_df['GR'] > 5000)\n",
    "high_GR = param_df2[param_df2['GR'] > 10000]\n",
    "high_GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_points = param_df.hvplot.points(x='x', y='y', color='GR').opts(tools=['hover', 'tap'])\n",
    "param_points = hv.Points(param_df, ['x', 'y'], 'GR').opts(color='GR', size=8, aspect='equal').opts(tools=['hover', 'tap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_points2(param):\n",
    "    return hv.Points(param_df, ['x', 'y'], param).opts(color=param, size=8, aspect='equal').opts(tools=['hover', 'tap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.opts.defaults(hv.opts.Points(nonselection_alpha=0.4))\n",
    "esri = tiles.ESRI().redim(x='x', y='y').opts(width=500, height=400, bgcolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esri*param_points2('RHOB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_stream = streams.Selection1D(source=param_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelled_callback(index):\n",
    "    if len(index) == 0:\n",
    "        return  hv.Text(x=min(param_df['x']),y=max(param_df['y']), text='Hei').opts(color='white')\n",
    "    first_index = index[0] # Pick only the first one if multiple are selected\n",
    "    row = param_df.iloc[first_index]\n",
    "    text = '%s : lat: %d, long: %d' % (row.WELLNAME, row.LATITUDE, row.LONGITUDE)\n",
    "    return hv.Text(x=row.x, y=row.y, text=text).opts(color='white')\n",
    "\n",
    "labeller = hv.DynamicMap(labelled_callback, streams=[selection_stream])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(esri * param_points * labeller).opts(hv.opts.Points(tools=['tap', 'hover'])).opts(width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = streams.PointerXY(x=0, y=0)\n",
    "\n",
    "def cursor(x, y):\n",
    "    return (hv.Box(x, y, 2e6/np.sqrt(2)) *\n",
    "            hv.Ellipse(x, y, 2e6)).opts(\n",
    "        hv.opts.Ellipse(color='yellow', alpha=0.7), hv.opts.Box(color='white', alpha=0.7)\n",
    "    )\n",
    "cursor_point = hv.DynamicMap(cursor, streams=[pointer])\n",
    "cursor_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = hv.Ellipse(0, 0, 4)\n",
    "poly_stream = streams.PolyDraw(source=poly, drag=True, num_objects=4,\n",
    "                               show_vertices=True)\n",
    "poly.opts(opts.Polygons(fill_alpha=0.3, active_tools=['poly_draw']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_well(index):\n",
    "    if len(index) == 0:\n",
    "        return  hv.Overlay([]) # Empty overlay if we have not clicked on a well\n",
    "    first_index = index[0] # Pick only the first one if multiple are selected\n",
    "    row = param_df.iloc[first_index]\n",
    "    return (hv.Box(row.x, row.y, 2e6/np.sqrt(2)) *\n",
    "            hv.Ellipse(row.x, row.y, 2e6)).opts(\n",
    "        hv.opts.Ellipse(color='yellow', alpha=0.7, line_width=3), hv.opts.Box(color='orange', alpha=0.7, line_width=3)\n",
    "    )\n",
    "\n",
    "well_marker = hv.DynamicMap(mark_well, streams=[selection_stream])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(esri * param_points.opts(tools=['tap']) * well_marker).opts(width=700, height=600, max_width=700, max_height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wells_around_point(df, lat, lon, degrees_dist=20):\n",
    "    #if not index:\n",
    "    #    return ds.iloc[[]]\n",
    "    #row = param_points.data.iloc[index[0]]\n",
    "    half_dist = degrees_dist/2\n",
    "    #df = ds.data\n",
    "    nearest = df[((df['LATITUDE'] - lat).abs() < half_dist) \n",
    "                 & ((df['LONGITUDE'] - lon).abs() < half_dist)]\n",
    "    #return hv.Dataset(nearest)\n",
    "    return nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = hv.Dataset(param_df)\n",
    "index_stream = Selection1D(source=param_points, index=[-10])\n",
    "\n",
    "filtered_ds = ds.apply(wells_around_point, index=index_stream.param.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_selection(indices, cache={}):\n",
    "    if not indices: \n",
    "        return param_df.iloc[[]]\n",
    "    index = indices[0]   # Pick only the first one if multiple are selected\n",
    "    if index in cache: return cache[index]\n",
    "    row = param_df.iloc[index]\n",
    "    selected_df = wells_around_point(big_df, row.LATITUDE, row.LONGITUDE)\n",
    "    cache[index] = selected_df # Save the selected wells in cache, so we don't have to reload the wells we select again\n",
    "    return selected_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.opts.defaults(\n",
    "    hv.opts.Histogram(toolbar=None),\n",
    "    hv.opts.Scatter(toolbar=None)\n",
    ")\n",
    "\n",
    "#def histogram(ds):\n",
    "#    return ds.hvplot.hist(y='GR', bin_range=(0, 10), bins=20, color='red', width=400, height=250)\n",
    "\n",
    "#def scatter(ds):\n",
    "#    return ds.hvplot.scatter('NPHI', 'WELLNAME', color='green', width=400, height=250, padding=0.1)\n",
    "\n",
    "def histogram_callback(index):\n",
    "    title = 'Distribution of all wells within half a degree of selection'\n",
    "    selected_df = index_to_selection(index)\n",
    "    return selected_df.hvplot.hist(y='GR', bins=20, color='red', title=str(len(selected_df)))\n",
    "\n",
    "histogram = hv.DynamicMap(histogram_callback, streams=[selection_stream])\n",
    "\n",
    "def scatter_callback(index):\n",
    "    title = 'Scattering of all wells within half a degree of selection '\n",
    "    selected_df = index_to_selection(index)\n",
    "    return selected_df.hvplot.scatter('GR', 'RHOB', color='WELLNAME', title=title)\n",
    "\n",
    "scatter = hv.DynamicMap(scatter_callback, streams=[selection_stream])\n",
    "\n",
    "def vline_callback(index):\n",
    "    if not index:\n",
    "        return hv.VLine(0)\n",
    "    row = param_df.iloc[index[0]]\n",
    "    return hv.VLine(row.NPHI).opts(line_width=1, color='black')\n",
    "\n",
    "scatt_vline = hv.DynamicMap(vline_callback, streams=[index_stream])\n",
    "\n",
    "#dynamic_scatter = param_df.apply(scatter, axis=1)\n",
    "#dynamic_histogram = filtered_ds.apply(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(\n",
    "    (esri * param_points.opts(tools=['tap']) * labeller * well_marker * cursor_point).opts(width=700, height=600, max_width=700, max_height=600),\n",
    "    pn.Row(\n",
    "        scatter * scatt_vline,\n",
    "        histogram\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
